---
title: "All Homework"
author: "Jianrui Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{All Homework}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Homework 1

## Question

Use knitr to produce at least 3 examples (texts, ﬁgures, tables).

---

## Answer

### Example 1: text

```{r}
ad_data = read.csv("advertising.csv")  ##Import the data
lg = lm(sales~TV,data = ad_data)  ## Linear Regression
summary(lg)$coef
```
The $R^2$ is `r summary(lg)$r.squared`

----

### Example 2: table

```{r}
knitr::kable(head(ad_data))    ##Transform to .Rmd
```

----

### Example 3: figure

```{r}
par(mfrow=c(1,1))   
plot(lg)            
```

---

# Homework 2

## Question

* Exercises 3.4, 3.11, and 3.18 (pages 94-96, Statistical Computating with R).

---

## Answer

### Exercise 3.4
The Rayleigh density is 
$$
f(x)= \frac{x}{\sigma^2} e^{−x^2/(2\sigma^2)},x\ge0,σ>0. 
$$
Develop an algorithm to generate random samples from a Rayleigh($\sigma$) distribution. Generate Rayleigh($\sigma$) samples for several choices of $\sigma>0$ and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram). 


$Sol.$ To generate random samples from a Rayleigh($\sigma$) distribution, we apply the inverse transfrom method.
Due to the probability density function of a Rayleigh($\sigma$) distribution, we obtain that its cumulative density function is 
$$
F_X (x) = 1- e^{−x^2/(2\sigma^2)},x\ge0,σ>0.
$$
If $u\in (0,1)$, then
$$
F_X^{-1}(u) = \sigma\sqrt{-2\log (1-u)}.
$$
Notice that if $U\sim U(0,1)$, then $V=1-U \sim U(0,1)$. Hence applying the inverse transform method, we obtain $-\sigma\sqrt{2\log V}$ has the same distribution as $X$.
```{r}
ge_rayleigh = function(n, sigma){ 
  #generate n random samples from Rayleigh(sigma)
  
  V = runif(n)
  return(sigma * sqrt(-2*log(V)) )
}

```

```{r}
set.seed(12)

hist_rayleigh = function(n, sigma, y){
  # Draw a histogram to check
  # y is a sequence
  
  x = ge_rayleigh(n, sigma)                      # Generate the random sample
  
  hist(x, prob = T,main="Rayleigh Distribution")
  
  lines(y, y*exp(-y^2/(2*sigma^2))/(sigma^2))
}

hist_rayleigh(100000,1,seq(0,5,0.05))         # Histogram when sigma = 1

hist_rayleigh(100000,3,seq(0,15,0.15))        # Histogram when sigma = 3

hist_rayleigh(100000,0.2,seq(0,1,0.01))       # Histogram when sigma = 0.2



```




----

### Exercise 3.11
Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0,1)$ and $N(3,1)$ distributions with mixing probabilities $p_1$ and $p_2 =1−p_1$. Graph the histogram of the sample with density superimposed, for $p_1 =0 .75$. Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures. 

$Sol.$ To generate the random sample, we applied the first algorithm on $\textit{Statistical Computing with R}$, Page 78. From the 9 graphs, we can guess that it is likely to produce bimodal mixtures when $p_1$ is near $0.5$.
```{r}
set.seed(27)

ge_NLM = function(n, p1){
    # Generate a random smaple of size n from a normal location mixture, with p_1 = p1.  
    # I applied the (first) algorithm on Statistical Computing with R, Page 78.
  
    U = runif(n)                                     
    
    X = rep(0,n)
    
    for(i in 1:n){
      
      if(U[i] <= p1){
        
        X[i] = rnorm(1)              # Generate X from N(0,1)
      
      }
      else{
        
        X[i] = 3 + rnorm(1)          # Generate X from N(3,1)
      
      }
    }
    return(X)
}

hist_NLM = function(n, p1){
  # Generate the sample and graph the histogram
  
  X = ge_NLM(n, p1)
  
  hist(X, probability = T, main = "Normal Location Mixture")             # Graph the histogram    
  
  y = seq(-4,6,0.1)
  
  lines(y, (p1*exp(-1/2 * y^2) + (1-p1)*exp(-1/2 * (y-3)^2))/sqrt(2*pi))      
  # graph the probability density function
  
}

hist_NLM(1000, 0.75)


for(p in seq(0.1,0.9,0.1)){
  hist_NLM(1000,p)                       # Repeat with different values
}


```


----

### Exercise 3.18
Write a function to generate a random sample from a $W_d (\Sigma,n)$ (Wishart) distribution for $n>d+1 \ge 1$, based on Bartlett’s decomposition. 

$Sol.$ To generate a random sample from a $W_d (\Sigma,n)$ (Wishart) distribution, we apply the algorithm on $\textit{Statistical Computing with R}$, Page 80. 
```{r}
library(matrixcalc)

ge_Wishart = function(n, Sigma){
  # Generate a random sample from a W_d(sigma,n) distribution.
  
  if(is.numeric(n) == F | length(n) > 1){      # Check that n and sigma satiesfy the assumptions needed.
    return("n is not a number")
  }
  
  n = as.integer(n)
  
  if(is.matrix(Sigma) == F){
    return("Sigma is not a matrix")
  }
  if(isSymmetric(Sigma) == F){
    return("Sigma is not symmetric")
  }
  if(matrixcalc::is.positive.definite(Sigma) == F){
    return("Sigma is not positive definite")
  }
  
  d = nrow(Sigma)
  
  if(n <= d+1){
    return("error in dimension")                # Check the assumption mentioned in the question.
  }
  
  L = chol(Sigma)                  
  # Obtain the Choleski factorization. Note that L is upper triangular.
  
  TT = matrix(0, nrow = d, ncol = d)
  # Create the matrix
  
  for(i in 1:d){                                
  #applied the algorithm on Statistical Computing with R, Page 80
    
    TT[i,i] = sqrt(rchisq(1, n-i+1))
    
    if(i > 1){
      
      for(j in 1:(i-1)){
        
        TT[i,j] = rnorm(1)
      } 
    }
  }
  
  return(t(L) %*% TT %*% t(TT) %*% L)
}


```
```{r}

Sigma = cbind(c(4,12,-16),c(12,37,-43),c(-16,-43,98))       # An example of Sigma

ge_Wishart(5,Sigma)       # Generate a random sample
```

---

#Homework 3

## Question

Exercises 5.1, 5.10, 5.15 (pages 149-151, Statistical Computating with R).

---

## Answer

### Exercise 5.1
Compute a Monte Carlo estimate of 
$$ 
\int_0^{\pi /3} \sin t\text{ d}t
$$
and compare your estimate with the exact value of the integral.

$Sol.$ First of all, we calculate the exact value of the integral.
$$
\int_0^{\pi /3} \sin t\text{ d}t = -\cos t|_{t=0}^{\pi /3} = \frac{1}{2}
$$
Then we compute a Monte Carlo estimate. Suppose $U \sim (0,\pi /3)$. Then its probability density function will be $\frac{3}{\pi}$ on $(0,\pi /3)$ and $0$ elsewhere. Hence, we have
$$
E(\sin U) = \int_0^{\pi /3} \frac{3}{\pi} \sin t\text{ d}t = \frac{3}{\pi}\int_0^{\pi /3} \sin t\text{ d}t
$$
Namely, to estimate the original integral, it suffice to estimate $\frac{\pi}{3}E(\sin U)$, where $U \sim (0,\pi /3)$.
```{r}
set.seed(1011)

U = runif(10000, min = 0, max = pi/3)     # generate 10000 random samples of U

pi / 3 * mean(sin(U))                     # Calculate the estimation of the integral
pi / 3 * mean(sin(U)) - 1/2               # calculate the bias
var(pi /3 * sin(U))                       # Calculate the sample variance
```

---

### Exercise 5.10
Use Monte Carlo integration with antithetic variables to estimate 
$$
\int_0^1 \frac{e^{-x}}{1+x^2} \text{d} x
$$
and ﬁnd the approximate reduction in variance as a percentage of the variance without variance reduction.

$Sol.$ Similar to the exercise above, we suppose $U \sim (0,1)$. Then we have 
$$
E(\frac{e^{-U}}{1+U^2}) = \int_0^1 \frac{e^{-x}}{1+x^2} \text{d}x
$$
Hence it suffice to estimate the expectation in the right-hand side of the equation above.
Following the codes on $\textit{Statistical Computing with R}$, Page 131, we construct a function that can  optionally compute the estimate with or without antithetic sampling. 
```{r}
MC.antithetic = function(R = 1000, antithetic = T){   
  # Modified from the codes on page 131
  
  u = runif(R/2)
  
  if(!antithetic){
    v = runif(R/2)
  }
    else{
    v = 1 - u  
  } 
  
  u = c(u,v)
  
  E = exp(-u)/(1+u^2)
  
  return(mean(E))
}


MC.antithetic(antithetic = T)   # estimate with antithetic sampling


m=1000

MC1 = rep(0,1000)
MC2 = rep(0,1000)

for(i in 1:1000){
  MC1[i] = MC.antithetic(antithetic = F)
  MC2[i] = MC.antithetic(antithetic = T)
}

sd(MC1)                         # sample variance without antithetic sampling

sd(MC2)                         # sample variance with antithetic sampling

(var(MC1)-var(MC2))/var(MC1)    # reduction in variance

```
Hence the approximated reduction in variance is `r (var(MC1)-var(MC2))/var(MC1)`.

---

### Exercise 5.15
Obtain the stratiﬁed importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

$Sol.$ First of all, we recall what we did in Example 5.10. We used importance sampling to estimate 
$$
\int_0^1 \frac{e^{-x}}{1+x^2} \text{d}x
$$
Our best result was obtained with importance function
$$
f_3(x)= \ \frac{e^{-x}}{1-e^{-1}},\quad 0<x<1
$$

And the integrand is
$$
g(x) = \frac{e^{-x}}{1+x^2} 1_{0<x<1}
$$
Then we copy the code ( of $f_3$) from Example 5.10.
```{r}
m <- 10000 

g <- function(x){ 
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1) 
}

u <- runif(m)                            #f3, inverse transform method 
x <- - log(1 - u * (1 - exp(-1))) 
fg <- g(x) / (exp(-x) / (1 - exp(-1))) 

theta.hat <- mean(fg)                    #estimate of importance sampling
se <- sd(fg)                             #estimated standard error
```
To carry out stratified importance sampling, we need to calculate the quantiles first. The cumulative distribution function of $f_3$ on $(0,1)$  is 
$$
F_3 (x) = \ \frac{1-e^{-x}}{1-e^{-1}},\quad 0<x<1 
$$
Following the instructions of Examples 5.13, we divide the interval $(0,1)$ into five subintervals, $(\frac{j}{5},\frac{j+1}{5})$, $j=0,1,2,3,4$. Then we can take
$$
F_3^{-1} (0) = 0, \ F_3^{-1} (1)=1
$$
and
$$
F_3^{-1} (\frac{j}{5}) = -\log  (1-\frac{(1-e^{-1})j}{5})
$$
This equation also holds for $j=0$ and $j=6$.

Then we carry out the implementation. To compare with the result above, we replicate $10000/5 = 2000$ times on each of the 5 strata.
```{r}
m = 2000                                  #replicate 2000 times on each strata 

Quantile=c(0,0.1,0.2,0.3,0.4,1)

# the following codes are modified from Example 5.12

fg_im = matrix(0,nrow = 5, ncol = m)              #initialization

for(j in 1:5){
  u = runif(m,(j-1)/5,j/5)       #f3, inverse transform method
  x = - log(1 - u * (1 - exp(-1))) 
  fg_im[j,] = g(x) / (exp(-x) / (1 - exp(-1))) 
}
  
fg2 = colMeans(fg_im)

theta.hat2 = mean(fg2) 
se2 = sd(fg2)

result = rbind(c(theta.hat,se),c(theta.hat2,se2))
rownames(result) = c("Importance Sampling","Stratified Importance Sampling")
colnames(result) = c("estimate","estimated standard error")

result

(var(fg)-var(fg2))/var(fg)          # reduction in variance
```
Hence the approximated reduction in variance is `r (var(fg)-var(fg2))/var(fg)`.

Remark: Notice that if $U \sim (\frac{j-1}{5},\frac{j}{5})$ ($j=0,\dots,4$) and $X$ is generated from $X=F_X^{-1}(U)$, then the density of $X$ will be $5f_X(x) , \ \frac{j-1}{5}<x<\frac{j}{5}$, our desired result in Example 5.13.

---

# Homework 4

## Question

Exercise 6.5,6.6 (page 180, $\textit{Statistical Computing With R}$).

---

## Answer

### Exercise 6.5
Suppose a $95\%$ symmetric $t$-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the $t$-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$. Compare your $t$-interval results with the simulation results in Example 6.4. (The $t$-interval should be more robust to departures from normality than the interval for variance.) 

$Sol.$  First of all, let us recall that the $t$-interval used to estimate a mean is $[\overline{X}-T_{\alpha/2}\frac{S}{\sqrt{n}},\overline{X}+T_{\alpha/2}\frac{S}{\sqrt{n}}]$. Meanwhile, the confidence interval for the variance in Example 6.4 is $(0,\frac{(n-1)S^2}{\chi_{.05}^2 (n-1)})$. 
```{r}
set.seed(1018)

m = 1e5                    #repeat 1e5 times
n = 20                     #sample size

estimate = rep(0,m)
estimate_2 = rep(0,m)

for(i in 1:m){
  
  X = rchisq(n, df = 2)       # generate random samples from  chi_2^2 
  
  estimate[i] = mean(X)- qt(0.975,n-1)*var(X)/sqrt(n) <= 2 & mean(X) + qt(0.975,n-1)*var(X)/sqrt(n) >= 2
  # notice that the mean of the chi_2^2 is 2
  # calculate whether the mean of chi_2^2 is 2 is covered by the t-interval in the ith simulation
  
  estimate_2[i] = (n-1)*var(X)/qchisq(0.05, df = n-1) >= 4
  # the variance of the chi_2^2 is 4
  # calculate whether the variance of chi_2^2 is covered by the confidence interval in Example 6.4
  
  }

mean(estimate)              # estimated coverage probability of the t-interval for mean
mean(estimate_2)            # estimated coverage probability of the confidence interval for variance in Example 6.4
```
If the samples are from a normal distributon, then both of the two estimations should be close to $0.95$. However, here the samples are taken from a $\chi^2(2)$ distribution, and we see that, same as the hint in the Exercise, the 
$t$-interval is more robust to departures from normality than the interval for variance.

---

### Exercise 6.6
Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_1} \approx N(0,6/n)$. 

$Sol.$ Recall that on page 165, the author stated that if $X$ is normal, then the sample coefficient of skewness (denoted by $\sqrt{b_1}$) is asymptotically normal with mean $0$ and variance $6/n$. However, on page 167, the author pointed out that for finite samples, one should use
$$
Var(\sqrt{b_1}) = \frac{6(n-2)}{(n+1)(n+3)}
$$
Here we give computations of both of the variances.
```{r}
q = c(0.025,0.05,0.95,0.975)

sk = function(x){ 
  #computes the sample skewness coeff. 
  xbar = mean(x) 
  m3 = mean((x - xbar)^3) 
  m2 = mean((x - xbar)^2) 
  return( m3 / m2^1.5 ) 
} 


Quantile_skew = function(mean = 0, sd = 1, n = 500, m = 1e4, q){
  # Generate the table of the answer
  
  # Input: mean and sd are mean and standard error of the normal distribution.
  # n is the sample size. m is the repeating times.
  # q is the sequence of the quantiles.
  
  skew = rep(0,m)
  
  for(i in 1:m){
  X = rnorm(n, mean = mean, sd = sd)          
  skew[i] = sk(X)                         # Calculating the sample skewness coefficients
  }
  
  a = quantile(skew, probs = q)           
  # the samples quantiles of the sample skewness coefficients          
  
  b_1 = qnorm(q, sd = sqrt(6/n) )         
  # theoretical quantiles of the sample skewness coefficients, with the first variance sqrt{6/n} applied
  
  b_2 = qnorm(c(0.025,0.05,0.95,0.975), sd = sqrt(6*(n-2)/(n+1)/(n+3)))
  # theoretical quantiles of the sample skewness coefficients, with the second variance applied
  
  c_1 = sqrt( q*(1-q)/n/( pnorm(b_1, sd = sqrt(6/n)) )^2 )
  # the standard error of estimates, with the first variance sqrt{6/n} applied
  
  c_2 = sqrt( q*(1-q)/n/( pnorm(b_2, sd = sqrt(6/n)) )^2 )
  # the standard error of estimates, with the second variance applied
  
  result = cbind(a,b_1,b_2,c_1,c_2)
  colnames(result) = c("sample quantile","theoretical quantile 1","theoretical quantile 2","standard error 1", "standard error 2")
  
  knitr::kable(result)
  
}

set.seed(123)
Quantile_skew(q = q)
Quantile_skew(n = 20, q=q)
```

We see that, when the sample size $n=500$, both of the variances give good results in calculating the quantiles. However, when $n$ is small, it is usually better to use 
$$
Var(\sqrt{b_1}) = \frac{6(n-2)}{(n+1)(n+3)},
$$
which has been suggested on Page 167. 

---

# Homework 5

## Question

Exercises 6.7, 6.A, slides page 22 (pages 180-181, Statistical Computating with R).

---

## Answer

### Exercise 6.7
Estimate the power of the skewness test of normality against symmetric $\text{Beta} (\alpha,\alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $\text{}t(\nu)$?

$Sol.$ This is similar to what we did in Example 6.10 (Page 169).  We just need to copy the code from Example 6.10 and do some modifications.
```{r}
sk = function(x) { 
  #computes the sample skewness coeff. 
  xbar = mean(x) 
  m3 = mean((x - xbar)^3) 
  m2 = mean((x - xbar)^2) 
  return( m3 / m2^1.5 ) 
} 


Pwr_Skew_NB = function(alpha, sl = 0.1, n = 30, m = 2500  ){
  # return the estimated power
  # alpha is the parameter in Beta distribution
  # sl is the significance level
  # n is the sample size
  # m is the replication times
  
  N = length(alpha) 

  pwr = numeric(N) 
 
  cv = qnorm(1-sl/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))      
  #critical value for the skewness test

  for (j in 1:N) {              #for each epsilon 
  
    shape = alpha[j] 
  
    sktests= numeric(m) 
  
    for (i in 1:m) {            #for each replicate 
    
      x = rbeta(n, shape1 = shape, shape2 = shape) 
    
      sktests[i] = as.integer(abs(sk(x)) >= cv) 
    
    } 
  
  pwr[j] = mean(sktests) 
  }
  
  return(pwr)
}

alpha = seq(0,2,0.1)
#plot power vs alpha
plot(alpha, Pwr_Skew_NB(alpha = alpha), type = "b", xlab = bquote(alpha), ylab = "power", ylim = c(0,0.1)) 
abline(h = .1, lty = 3)


alpha = seq(2,20,1)
#plot power vs alpha
plot(alpha, Pwr_Skew_NB(alpha = alpha), type = "b", xlab = bquote(alpha), ylab = "power", ylim = c(0,0.1)) 
abline(h = .1, lty = 3)

```

We found that the power of the tests are really small. Using this test, it is unlikely for us to reject the hypothesis that the distribution is normal, even when a Beta distribution is used.


```{r}
Pwr_Skew_NT = function(alpha, sl = 0.1, n = 30, m = 2500  ){
  # return the estimated power
  # alpha is the parameter in t distribution
  # sl is the significance level
  # n is the sample size
  # m is the replication times
  
  N = length(alpha) 

  pwr = numeric(N) 
 
  cv = qnorm(1-sl/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))      
  #critical value for the skewness test

  for (j in 1:N) {              #for each epsilon 
  
    freedom = alpha[j] 
  
    sktests= numeric(m) 
  
    for (i in 1:m) {            #for each replicate 
    
      x = rt(n, df = freedom) 
    
      sktests[i] = as.integer(abs(sk(x)) >= cv) 
    
    } 
  
  pwr[j] = mean(sktests) 
  }
  
  return(pwr)
}

alpha=1:20
#plot power vs alpha
plot(alpha, Pwr_Skew_NT(alpha = alpha), type = "b", xlab = bquote(alpha), ylab = "power", ylim = c(0,1)) 
abline(h = .1, lty = 3) 
```

For heavy-tailed distribution $\text{}t(\nu)$, the power is big when $\alpha$ is small. However, when $\alpha$ gets large, $\text{}t(\nu)$ converges to standard normal random variable in distribution, and accordingly, the power gets smaller.

---

### Projects 6.A
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the $t$-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The $t$-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is:
(i) $\chi^2 (1)$, (ii) Uniform$(0,2)$, and (iii) Exponential(rate$=1$). In each case, test $H_0 : \mu = \mu_0$ vs $H_1 : \mu\neq \mu_0$, whereµ0 is the mean of $\chi^2 (1)$, Uniform$(0,2)$, and Exponential($1$), respectively.

$Sol.$
(i) $\chi^2 (1)$
```{r}

Type_1_chisq = function(sl, df = 1, m = 1e4, n = 50){
  # reutrn the empirical Type I error rate of chi square distribution
  # sl is the significance level(sl can be a sequence)
  # df is the degree of freedom
  # m is the number of the replication times
  # n is the sample size
  
  N = length(sl)
  result = rep(0,N)
  
  for(j in 1:N){
    
    esl = rep(0,n) 

    for(i in 1:m){
      x = rchisq(n, df = 1)                                               # generate the random sample
      xbar = mean(x)
      q = qt(1-sl[j]/2, df = n-1)                                       # calculate the quantile
      S = var(x)
      esl[i] = mean(xbar-q*S/sqrt(n) > 1 || xbar+q*S/sqrt(n) < 1 )        
      # calculate empirical Type I error rate
      # the mean of chi^2(1) is 1
    }
    result[j] = mean(esl)
  }
  return(result)
}

sl = c(0.05,0.1,0.5)
M = rbind(sl, Type_1_chisq(sl))
rownames(M) = c("significance level","empirical type I error rate")
M
```
We found that empirical type I error rate is rather different from the significance level.

(ii) Uniform$(0,2)$
```{r}

Type_1_unif = function(sl, minimum = 0, maximum = 2, m = 1e4, n = 30){
  # reutrn the empirical Type I error rate of uniform distribution
  # sl is the significance level(sl can be a sequence)
  # minimum and maximum is the minimum and maximum of the uniform distribution
  # m is the number of the replication times
  # n is the sample size
  
  N = length(sl)
  result = rep(0,N)
  
  for(j in 1:N){
    
    esl = rep(0,n) 

    for(i in 1:m){
      x = runif(n, min = minimum, max = maximum)                                  # generate the random sample
      xbar = mean(x)
      q = qt(1-sl[j]/2, df = n-1)                                       # calculate the quantile
      S = var(x)
      esl[i] = mean(xbar-q*S/sqrt(n) > 1 || xbar+q*S/sqrt(n) < 1 )        
      # calculate empirical Type I error rate
      # the mean of chi^2(1) is 1
    }
    result[j] = mean(esl)
  }
  return(result)
}

sl = c(0.05,0.1,0.5)
M = rbind(sl, Type_1_unif(sl))
rownames(M) = c("significance level","empirical type I error rate")
M
```
We found that empirical type I error rate is apparently different from the significance level.


(iii) Exponential($1$)
```{r}
Type_1_exp = function(sl, rate = 1, m = 1e4, n = 30){
  # reutrn the empirical Type I error rate of uniform distribution
  # sl is the significance level(sl can be a sequence)
  # rate is the parameter in exponential distribution
  # m is the number of the replication times
  # n is the sample size
  
  N = length(sl)
  result = rep(0,N)
  
  for(j in 1:N){
    
    esl = rep(0,n) 

    for(i in 1:m){
      x = rexp(n, rate = rate)                                 # generate the random sample
      xbar = mean(x)
      q = qt(1-sl[j]/2, df = n-1)                                       # calculate the quantile
      S = var(x)
      esl[i] = mean(xbar-q*S/sqrt(n) > 1 || xbar+q*S/sqrt(n) < 1 )        
      # calculate empirical Type I error rate
      # the mean of chi^2(1) is 1
    }
    result[j] = mean(esl)
  }
  return(result)
}

sl = c(0.05,0.1,0.5)
M = rbind(sl, Type_1_exp(sl))
rownames(M) = c("significance level","empirical type I error rate")
M
```
We found that empirical type I error rate is rather different from the significance level.

Based on the Monte Carlo estimates above, we can say the empirical type I error rate have large difference from the significance level in the case of uniform distribution. However, in the other two cases, the difference varies when the significance level changes. 

---

### Discussion

    + If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?
        
        - What is the corresponding hypothesis test problem?
        
        - What test should we use? Z-test,  two-sample t-test, paired-t test or McNemar test?
        
        - What information is needed to test your hypothesis?

$Sol.$ (i) For power functions $\pi_1$ and $\pi_2$, we need to test hypothesis $H_0 : \pi_1(\theta) = \pi_2 (\theta)$ against $H_1 : \pi_1(\theta) \neq \pi_2 (\theta) $.

(ii) I first exlude the McNemar test because we may not have the result in a contiency table. In this question we only know the estimated powers.
Then I prefer a $t$-test to a $Z$-test since the distribution of the empirical power may be unable to be approximated by a normal distribution.
Finally, the unpaired $t$ test is more practical because the two samples of the powers may not be paired. In this question maybe we only know information (mean, variance, etc.) about these two separate samples.

(iii) We just need the two sample variances to carry out a unpaired $t$-test (Welch's t-test).

---

# Homework 6

## Question

Exercise 7.6 and Project 7.B(Statistical Computing with R, pages 212-213).

---

## Answer

### Exercise 7.6
Efron and Tibshirani discuss the $scor (bootstrap)$ test score data on 88 students who took examinations in ﬁvesubjects [84, Table 7.1], [188, Table 1.2.1]. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores $(x_{i1},\dots,x_{i5})$ for the $i$th student. Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat{\rho}_{12}=\hat{\rho}(\text{mec, vec})$, $\hat{\rho}_{34}=\hat{\rho}(\text{alg, ana})$, $\hat{\rho}_{35}=\hat{\rho}(\text{alg, sta})$, $\hat{\rho}_{45}=\hat{\rho}(\text{ana, sta})$. 

$Sol.$
```{r}
library(bootstrap)
k = ncol(bootstrap::scor)
names = colnames(bootstrap::scor)
par(mfrow = c(2, 3))
for(i in 1:(k-1)){
  for(j in (i+1):k){
    plot(bootstrap::scor[,i],bootstrap::scor[,j],sub = paste("correlation","=",cor(bootstrap::scor[,i],bootstrap::scor[,j])), xlab=paste(names[i]),ylab=paste(names[j]) )
  }
}
```

When the correlation is closer to $1$, the dots on the scatter plot is more similar to a line with slope$=1$.
```{r}
par(mfrow = c(1, 1))


rho = function(x,i){
  return(cor(x[i,1],x[i,2]))
}

library(boot)

#boostrap estimate of cor(mec,vec)
sd(boot::boot(data = scor[,c(1,2)], statistic = rho, R=2000)$t)

#boostrap estimate of cor(alg,ana)            
sd(boot::boot(data = scor[,c(3,4)], statistic = rho, R=2000)$t)

#boostrap estimate of cor(alg,sta)
sd(boot::boot(data = scor[,c(3,5)], statistic = rho, R=2000)$t)

#boostrap estimate of cor(ana,sta)
sd(boot::boot(data = scor[,c(4,5)], statistic = rho, R=2000)$t)
```

---

### Project 7.B
Repeat Project 7.A for the sample skewness statistic. Compare the coverage rates for normal populations (skewness $0$) and $\chi^2(5)$ distributions (positive skewness).
$Sol.$
```{r}
sk = function(x) { 
  #computes the sample skewness coeff. 
  xbar = mean(x) 
  m3 = mean((x - xbar)^3) 
  m2 = mean((x - xbar)^2) 
  return( m3 / m2^1.5 ) 
}

sk_boot= function(x,i){
  #function used for bootstrap 
  return( sk(x[i,]) )
}


sk_cp_norm = function(n = 30, m = 1000, R = 200, conf = 0.95){
  # return the four estimated coverage probabilities of the four intervals for nomal populations
  # m is the repeating times of this estimation
  # R is the number of bootstrap replicates
  # conf is the confidence level
  
  cp_norm = rep(0,m)
  cp_perc = rep(0,m)
  cp_basic = rep(0,m)
  
  
  for(i in 1:m){
    
    x = as.matrix(rnorm(n))
    
    obj = boot.ci(boot(data = x, statistic = sk_boot, R =R), conf = conf, type = c("norm", "basic", "perc"))
    
    # the skewness of a normal distribution is 0
    cp_norm[i] = obj$norm[2] <= 0 && obj$norm[3] >= 0
    cp_perc[i] = obj$perc[4] <= 0 && obj$perc[5] >= 0
    cp_basic[i] = obj$basic[4] <= 0 && obj$basic[5] >= 0
    
  }
  
  result =c(mean(cp_norm),mean(cp_perc),mean(cp_basic))
  return(result) 
}

sk_cp_chisq = function(n = 30, m = 1000, R = 200, conf = 0.95){
  
  cp_norm = rep(0,m)
  cp_perc = rep(0,m)
  cp_basic = rep(0,m)
  
  
  for(i in 1:m){
    
    x = as.matrix(rchisq(n, df = 5))
    
    obj = boot.ci(boot(data = x, statistic = sk_boot, R =R), conf = conf, type = c("norm", "basic", "perc"))
    
    # the skewness of chi square distribution  is 2*sqrt(2/degrees of freedom)
    cp_norm[i] = obj$norm[2] <= 2*sqrt(2/5) && obj$norm[3] >= 2*sqrt(2/5)
    cp_perc[i] = obj$perc[4] <= 2*sqrt(2/5) && obj$perc[5] >= 2*sqrt(2/5)
    cp_basic[i] = obj$basic[4] <= 2*sqrt(2/5) && obj$basic[5] >= 2*sqrt(2/5)
    
  }
  
  result = c(mean(cp_norm),mean(cp_perc),mean(cp_basic))
  return(result) 
}

compare = function(n = 30, m = 1000, R = 200, conf = 0.95){
  # compare the results and construct a table
  r1 = sk_cp_norm(n=n, m=m, R=R, conf = conf)
  r2 = sk_cp_chisq(n=n, m=m, R=R, conf = conf)
  result = rbind(r1,r2)
  rownames(result) = c("normal","chi sqaure")
  colnames(result) = c("normal","precentile","basic")
  knitr::kable(result)
}

compare(n = 30, m=1000, R = 200, conf = 0.95)
compare(n = 30, m=1000, R = 200, conf = 0.9)
compare(n = 30, m=1000, R = 200, conf = 0.5)
```

We found that the coverage rates for the normal distribution is much closer to the confidence level we set, comparing to that for the chi square distribution.

---

# Homework 7

## Question

Exercise 7.8 and 7.10 (Statistical Computing with R, page 213).

## Answer

### Exercise 7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

$Sol.$ 
```{r}
theta2 = function(xdata){                     # function used to calculate theta.hat
  vec = eigen(cov(xdata))$values
  return(vec[1]/sum(vec))
}

library(bootstrap)
xdata = as.matrix(scor)
n = nrow(xdata)

theta.hat = theta2(xdata)

theta.jack=numeric(n)
for(i in 1:n){
  theta.jack[i] = theta2(xdata[-i,])         
}

(n-1) * (mean(theta.jack) - theta.hat)          #bias of Jacknife

sqrt((n-1)/n * sum((theta.jack - mean(theta.jack))^2))        #standard error of Jacknife
```



### Exercise 7.10
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?

$Sol.$ We copy the code from Example 7.18 and replace the Log-log model with the cubic polynomial model.
```{r}
library(lattice)
library(DAAG) 
attach(ironslag)

n = length(magnetic) #in DAAG ironslag 
e1 = e2 = e3 = e4 = numeric(n)

# for n-fold cross validation # fit models on leave-one-out samples 
for (k in 1:n) { 
  y = magnetic[-k] 
  x = chemical[-k]
  
  J1 = lm(y ~ x) 
  yhat1 = J1$coef[1] + J1$coef[2] * chemical[k] 
  e1[k] = magnetic[k] - yhat1

  J2 = lm(y ~ x + I(x^2)) 
  yhat2 = J2$coef[1] + J2$coef[2] * chemical[k] + J2$coef[3] * chemical[k]^2 
  e2[k] = magnetic[k] - yhat2

  J3 = lm(log(y) ~ x) 
  logyhat3 = J3$coef[1] + J3$coef[2] * chemical[k] 
  yhat3 = exp(logyhat3) 
  e3[k] = magnetic[k] - yhat3
  
  # cubic polynomial model
  J4 = lm(y ~ poly(x,3,raw=T)) 
  yhat4 = J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
  e4[k] = magnetic[k] - yhat4

} 

c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)) 
```
By the cross-validation procedure, we should select the quadratic model.
```{r}
y = magnetic
x = chemical
r = rep(0,4)

L1 = lm(y~x)
r[1] = summary(L1)$adj.r.squared # extract adjusted R^2

L2 = lm(y ~ x + I(x^2))
r[2] = summary(L2)$adj.r.squared

L3 = lm(log(y) ~ x) 
r[3] = summary(L3)$adj.r.squared

L4 = lm(y ~ poly(x,3,raw=T)) 
r[4] = summary(L4)$adj.r.squared

r
```

We also should choose the quadratic model according to maximum adjusted $R^2$.

---

# Homework 8

## Question

Exercise 8.3 (Statistical Computing with R, page 243) and slides page 31.

----

## Answer

### Exercise 8.3
The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

$Sol.$
If $X$ and $Y$ have unequal sample sizes, we assume the the sample size of $X$ is $m$, the sample size of $Y$ is $N$, and , without loss of generality, $m<n$. Then, for $Z=c(X,Y)$, we let the statistic 
$$
\hat{\theta}(Z,\nu) = maxout(Z[1:m],Z[(m+1):2m])
$$  
and do a permutation test. (See Page 177 on $\textit{Statistical Computing with R}$ for details of $maxout$.) 
Notice that this statistic favours $H_a$ when $\theta$ is large.(See page 7 of Prof. Zhang's ppt for details.)
Then 
```{r}
maxout <- function(x, y) { 
  # copied from Page 177
  X <- x - mean(x) 
  Y <- y - mean(y) 
  outx <- sum(X > max(Y)) + sum(X < min(Y)) 
  outy <- sum(Y > max(X)) + sum(Y < min(X)) 
  return(max(c(outx, outy))) 
}

per_count5test = function(x,y,N=999, alpha = 0.05){
  # N is the number of the permutations
  # alpha is the significance level
  # return 0 if we accept that X and Y are from same distribution(accept H_0), and return 1 otherwise(reject H_0) 
  
  if(length(x)<=length(y)){
    m = length(x)
    n = length(y)
    z = c(x,y)
    theta = maxout(x,y)
  }
  else{     
    #if X has more samples than Y, we exchange them
    m = length(y)
    n = length(x)
    z = c(y,x)
    theta = maxout(y,x)
  }
  
  theta_hat = rep(0,N)
  
  for(i in 1:N){
    k = sample(1:(m+n), size = 2*m, replace = F)
    z1 = z[k]
    x1 = z1[1:m]
    y1 = z[(m+1):(2*m)]
    
    # Calculate the maxout for every permutation
    theta_hat[i] = maxout(x1, y1)
  }
  
  # calculate the empirical p value, see Page 217 for details
  p = (1+sum(theta_hat>=theta))/(N+1)
  
  # alpha = 0.05 is the desired significance level
  return(as.integer(p <= alpha))
}
```
We then redo exmaple 6.15 and calculate the empirical I error.
```{r}
n1 = 20
n2 = 30
m = 1e4
D = rep(0,m)
for(i in 1:m){
  D[i] = per_count5test(rnorm(n1),rnorm(n2))
}
mean(D)
```
The result is bad...

Finally, We give an example in estimating the power of the test. Here the sample distribtion are $N(0,1)$ and $\chi_1^2$.
```{r}
n1 = 20
n2 = 30
m = 1e4
D = rep(0,m)
for(i in 1:m){
  D[i] = per_count5test(rnorm(n1),rchisq(n2, df=1))
}
# estimate the power
mean(D)
```


----

### slides page 31

* Power comparison (distance correlation test versus ball covariance test)

    + Model 1: $Y=X/4+e$
    + Model 2: $Y=X/4\times e$
    + $X\sim N(0_2,I_2)$, $e\sim N(0_2,I_2)$, $X$ and $e$ are independent.

$Sol.$ 

```{r}
library(Ball)
library(energy)


pow_comparison = function(n, N=1e3, alpha = 0.05){
  # return the estimated power of ball test and distance correlation test
  # n is the sample size
  # N is the replication times
  # alpha is the significance level
  
  p_bd = rep(0,n)
  p_dcor = rep(0,n)
  
  for(i in 1:N){
    X = matrix(rnorm(2*n), nrow = n, ncol = 2)
    e = matrix(rnorm(2*n), nrow = n, ncol = 2)
    Y1 = X/4 + e
    Y2 = X/4 * e
    p_bd[i] = bd.test(x = Y1, y = Y2, seed = i*n^2, R = 99)$p.value
    p_dcor[i] = dcor.test(x = Y1, y = Y2, R = 99)$p.value
  }
  return(c(mean(p_bd < alpha), mean(p_dcor < alpha)))
}


n = seq(5, 100, 5)
pow = matrix(0, nrow = length(n), ncol = 2)
for(i in 1:length(n)){
  pow[i,] = pow_comparison(n[i])
}
pow = cbind(n,pow)
colnames(pow) = c("sample size", "ball test", "dcor test")
knitr::kable(pow)
```
```{r}
plot(n, pow[,2], type="l", col = "blue", lty=5, ylim = c(0,1), ylab="power")
lines(n, pow[,3], type = "l", col = "red", lty =3)
legend("bottomright", c("ball","dcor"), fill = c("blue","red"))
```
We found that the power of ball tests is larger than the power of the distance correlation test.

---

# Homework 9

## Question

Exercise 9.4 (Statistical Computing with R, page 277).

## Answer

### Exercise 9.4
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain. 


$Sol.$  We modify the codes from Example 9.1, following the instructions in the exercise. The proposal distribution $g(y|x)$ must be $N(\mu =x, \sigma^2)$ for some fixed $\sigma^2$. In addition, notice that $g(x|y)=g(y|x)$, which implies we can apply the property of the Metropolis sampler, which is stated in section 9.2.2.
```{r}
f = function(x){
  # return the density of standard Laplace distribution
  return(exp(-abs(x))/2)
}


Metropolis = function(m = 10000, sd = 1){
  # m is the number of replications
  # sd is the standard error of the normal distribution
  # k is the number of the rejected candidate points
  # x is the Markov Chain we obtain 
  
  x = numeric(m)
  
  #simulate from normal distribution
  x[1] = rnorm(1, mean = 0, sd = sd) 
  k = 0 
  u = runif(m)
  for (i in 2:m) { 
    xt = x[i-1] 
    y = rnorm(1, mean = xt, sd = sd)
    num = f(y)    # here it is the Metropolis sampler, see P253
    den = f(xt) 
    if (u[i] <= num/den){
      x[i] = y
    } 
    else { 
      x[i] = xt 
      k = k+1 #y is rejected 
    } 
  }
  return(list(x = x, k = k, sd = sd, m=m))
}

set.seed(1129)
#sd = 1
result = Metropolis(m = 10000, sd = 1)
1 - result$k / result$m # acceptance rate
index = 5000:5500 
y1 = result$x[index] 
plot(index, y1, type="l", main="sd=1", ylab="x") 

#sd = 10
result = Metropolis(m = 10000, sd = 10)
1 - result$k / result$m # acceptance rate
index = 5000:5500 
y1 = result$x[index] 
plot(index, y1, type="l", main="sd=10", ylab="x") 

#sd = 0.1
result = Metropolis(m = 10000, sd = 0.1)
1 - result$k / result$m # acceptance rate
index = 5000:5500 
y1 = result$x[index] 
plot(index, y1, type="l", main="sd=0.1", ylab="x") 
```

We find that when $\sigma^2$ increases, the Markov Chain changes less rapidly, and the acceptance rate increases.

---

# Homework 10

## Question

* Exercise 11.1 and 11.5 (Statistical Computing wtih R, pages 353-354)

* A-B-O blood type probelm

## Answer

### Exercise 11.1
The natural logarithm and exponential functions are inverses of each other, so that mathematically log(exp$x$) $=$ exp(log$x$)$=$ $x$. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality? (See all.equal.)

$Sol.$
```{r}
x = rep(T,10)
for(i in 1:10){
  x[i] = log(exp(i))-exp(log(i))==0
}
x
```
Notice that the result is not always 'TRUE'. So this property does not always hold in computer arithmetic.
```{r}
x = rep(T,10)
for(i in 1:10){
  x[i] = all.equal(log(exp(i))-exp(log(i)), 0)
}
x
```
The result is always 'TRUE', which means the identity holds with near equality.



### Exercise 11.5
Write a function to solve the equation
$$
\frac{2\Gamma(\frac{k}{2})}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}{2})}\int_0^{c_{k-1}}(1+\frac{u^2}{k-1})^{-k/2}du 
= 
\frac{2\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}{2})}\int_0^{c_{k}}(1+\frac{u^2}{k})^{-(k+1)/2}du
$$
for $a$, where
$$
c_k = \sqrt{\frac{a^2 k}{k+1-a^2}}
$$

Compare the solutions with the points $A(k)$ in Exercise 11.4.

$Sol.$ First of all we give a function to calculate the points $A(k)$ in Exercise 11.4.
```{r}
SS = function(a, k){
  # the function S_k in Exercise 11.4
  1 - pt(sqrt((a^2 *k)/(k+1-a^2)), df = k)
}


AA = function(k){
  # calculate the points A(k) in Exercise 11.4
  # k CANNOT be a vector
  
  # I found that the point is usually between 1 and 2... 
  uniroot(function(x) SS(x,k-1)-SS(x,k) , c(1,min(sqrt(k)-0.01,2)))$root    
}
```

Then we write a function to solve the equation in Exercise 11.5, without taking advantage of the truth that the integrands in the equation are similar to $dt(u, df = k-1)$ and $dt(u, df = k-1)$ respectively.

```{r}
dt_new = function(x, df){
  # return the integrand in RHS of the equation in Exercise 11.5
  # return the same value as 2*dt(x, df)

  2*(1+x^2 /df)^(-(df+1)/2) /sqrt(pi*df) *exp(lgamma((df+1)/2)-lgamma(df/2))
}

III = function(a,k){
  # calculate the integral in the equation in Exercise 11.5
  
  c = sqrt(a^2 *k/(k+1-a^2))
  res = integrate(function(x) dt_new(x,k), lower = 0, upper = c)$value
  return(res)         
}

BB = function(k){
  # calculate the solution in Exercise 11.5
  # k CANNOT be a vector
  
  uniroot(function(x) III(x,k-1)-III(x,k) , c(1,min(sqrt(k)-0.01,2) ))$root
}
```
Finally, we can give a comparison.
```{r}
K = c(c(4:25))
compare = rep(0,length(K))
for(i in 1:length(K)){
  compare[i] = all.equal(AA(K[i]),BB(K[i]))
}
max(compare) == 1 #return T iff A(k)=B(k) for all the k above
```

### A-B-O blood type probelm

* Let the three alleles be A, B, and O with allele frequencies $p$, $q$, and $r$. The 6 genotype frequencies under HWE and complete counts are as follows. (Open the pdf to find the table)

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l|l|l|l|l|l}
\hline 
  Genotype & AA & BB & OO & AO & BO & AB & Sum \\ \hline
  Frequence & $p^2$ & $q^2$ & $r^2$ & $2pr$ & $2qr$ & $2pq$ & $1$ \\ \hline
  Count & $n_{AA}$ & $n_{BB}$ & $n_{OO}$ & $n_{AO}$ & $n_{BO}$ & $n_{AB}$ & $n$ \\ \hline
\end{tabular}
\end{table}

* Observed data:

\begin{align}
n_A =& \ n_{AA} +n_{AO}=28\quad \text{(A-type)} \notag \\
n_B =& \ n_{BB} +n_{BO}=24\quad \text{(B-type)} \notag \\
n_{OO} =& \ 41\quad \text{(O-type)} \notag \\
n_{AB} =& \ 70\quad \text{(AB-type)} \notag 
\end{align}

* Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{AA}$ and $n_{BB}$)

* Show that the log-maximum likelihood values in M-steps are increasing via line plot.

$Sol.$

* Observed data likelihood
\begin{align}
L(p,q|n_A,n_B,n_{OO},n_{AB}) =& \ (p^2+2pr)^{28} (q^2+2qr)^{24} (r^2)^{41} (2pq)^{70} \notag \\
=& \ (2p-p^2-2pq)^{28} (2q-q^2-2pq)^{24} (1-p-q)^{82} (2pq)^{70} \notag \\
=& \ (2-p-2q)^{28} (2-q-2p)^{24} (1-p-q)^{82} p^{98} q^{94} \notag
\end{align}

When the observed data likelihood attains its maximum, we have
```{r}
ll = function(theta){
  p = theta[1]
  q = theta[2]
  -(28*log(2-p-2*q) +24*log(2-q-2*p) + 92*log(1-p-q) + 98*log(p) + 94*log(q))
}

optim(c(0.3,0.3), ll)
```

To perform the EM algorithm, we need to do more calculations.

* Observed data likelihood
$$
L(p,q|n_{AA},n_{BB},n_A,n_B,n_{OO},n_{AB}) = (p^2)^{n_{AA}} (2pr)^{28-n_{AA}} (q^2)^{n_{BB}} (2qr)^{24-n_{BB}} (r^2)^{41} (2pq)^{70}
$$
$$
l(p,q|n_{AA},n_{BB},n_A,n_B,n_{OO},n_{AB}) = (98+n_{AA}) \log p + (94+n_{BB}) \log q + (134-n_{AA}-n_{BB}) \log (1-p-q) + C
$$

\begin{align}
E_{p_n} l
= & \ (98-\frac{28p_n}{2-p_n-2q_n}) \log p + (94-\frac{24q_n}{2-q_n-2p_n}) \log q \notag \\
& + (134-\frac{28p_n}{2-p_n-2q_n}-\frac{24q_n}{2-q_n-2p_n}) \log (1-p-q) + C \notag \notag 
\end{align}

```{r}
l = function(theta, p_0,q_0){
  p = theta[1]
  q = theta[2]
  r = 1-p-q
  nAA = 28*p_0/(2-p_0-2*q_0)
  nBB = 24*q_0/(2-q_0-2*p_0)
  -((98+nAA)*log(p) + (94+nBB)*log(q) + (134-nAA-nBB)*log(r) )
}

# initialization
N = 10
Theta = matrix(0, ncol = 2, nrow = N+1)
Theta[1,] = c(0.3,0.3)

# EM algorithm
for(i in 1:N){
  Theta[i+1,] = optim(c(0.4,0.4), function(theta) l(theta, p_0 = Theta[i,1], q_0 = Theta[i,2]))$par
}

# result
Theta[N+1,]
ll(Theta[N+1,])

x = 1:N
y = rep(0,N)
for(i in 1:N){
  y[i] = -ll(Theta[i,])
}
plot(x,y, type="l", col="red")

```


The result is strange...



---

# Homework 12

## Question

You have already written an R function for Exercise 9.4 (page 277, Statistical Computing with R). Rewrite an Rcpp function for the same task. 

1. Compare the generated random numbers by the two functions using qqplot. 

2. Campare the computation time of the two functions with microbenchmark. 

3. Comments your results.

## Answer

```{r,eval=FALSE}

#include <cmath>
#include <Rcpp.h>
using namespace Rcpp;

//[[Rcpp::export]]
double f(double x) {
  return exp(-abs(x));
}

//[[Rcpp::export]]
NumericVector Metropolis_C (int N, double sd) {
  NumericVector x(N);
  NumericVector initial = rnorm(1,0,sd);
  x[0] = initial[0];
  NumericVector u = runif(N);
  for (int i = 1; i < N;i++ ) {
    NumericVector y = rnorm(1, x[i-1], sd);
    if (u[i] <= (f(y[0]) / f(x[i-1]))){
      x[i] = y[0];
    }
    else {
      x[i] = x[i-1];
    }
  }
  return(x);
}

```

## comparison of the computation time

```{r}
library(Rcpp)
library(microbenchmark)

# R
f_R = function(x){
    # return the density of standard Laplace distribution
    return(exp(-abs(x))/2)
}


Metropolis_R = function(m = 10000, sd = 1){
  # m is the number of replications
  # sd is the standard error of the normal distribution
  # k is the number of the rejected candidate points
  # x is the Markov Chain we obtain 
  
  x = numeric(m)
  
  #simulate from normal distribution
  x[1] = rnorm(1, mean = 0, sd = sd) 
  k = 0 
  u = runif(m)
  for (i in 2:m) { 
    xt = x[i-1] 
    y = rnorm(1, mean = xt, sd = sd)
    num = f_R(y)    # here it is the Metropolis sampler, see P253
    den = f_R(xt) 
    if (u[i] <= num/den){
      x[i] = y
    } 
    else { 
      x[i] = xt 
      k = k+1 #y is rejected 
    } 
  }
  return(list(x = x, k = k, sd = sd, m=m))
}

    dir_cpp = 'D:/Statistical_Computing/Homework/Homework_12/'
    sourceCpp(paste0(dir_cpp,"Metropolis_C.cpp"))
    N = 10000
    sd = 2
    (time = microbenchmark(rwR=Metropolis_R(N,sd),rwC=Metropolis_C(N,sd)))
```

We can see that the running time of using Cpp functionis much shorter than using R function. So using Rcpp method can improve computing efficiency.

## qqplot

```{r}

set.seed(12345)
rwR = Metropolis_R(N,sd)$x[-(1:1000)]
rwC = Metropolis_C(N,sd)[-(1:1000)]
qqplot(rwR,rwC)
abline(a=0,b=1,col='black')
```

The dots of qqplot are located close to the diagonal lines. The random numbers generated by the two functions are similar.

